# 感知机
f(x)=sign(wx+b)</br>
w是权值，b是偏置，wx是w和x的内积，sign是符号函数；</br>
# k近邻法
k近邻模型三要素：距离度量、k值选择、分类决策规则;</br>
# 朴素贝叶斯法
# 决策树
描述对实例类型分类的树形结构，决策树由节点和有向边组成。内部节点表示一个特征或者属性，叶节点表示一个类。</br>
特征选择：</br>
## 信息增益：
熵：表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大。</br>
条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性。</br>
经验熵和经验条件熵：当熵和条件熵的概率由数据估计得到，所对应的熵和条件熵。</br>
信息增益：特征A对训练数据集D的信息增益g（D，A），定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差。
### ID3算法
在决策树的各个节点上应用信息增益准则选择特征，递归构建决策树。从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点，对子节点继续上述方法，构建决策树。相当于极大似然法进行概率模型选择。
### C4.5算法
生成过程中，用信息增益比来选择特征。
## 决策树剪枝
对于可能存在的过拟合现象，对已生成的决策树进行简化，该过程称为剪枝。
## CART算法
分类与回归树，既可用与分类也可用于回归。
# 逻辑斯蒂回归与最大熵模型
## 逻辑斯蒂分布
设X是连续随机变量，X服从逻辑斯蒂分布指X具有以下分布函数和密度函数
## 二项逻辑斯蒂回归模型
分类模型，由条件概率表示，形式为参数化的逻辑斯蒂分布
### 多项逻辑斯蒂回归模型
### 最大熵模型
熵最大的模型是最好的模型，通常用约束条件来确定概率模型的集合
# 模型学习的最优化算法
逻辑斯蒂回归模型、最大熵模型学习是以似然函数为目标的最优化问题，通常使用迭代算法求解。  常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法
## 改进的迭代尺度法IIS
假设最大熵模型当前参数向量w，我们希望找到一个w+o使得模型的对数似然函数值增大。使用一个方法进行w—>w+o，直到找到似然函数的最大值。
## 拟牛顿法
# 支持向量机
二分类模型，定义在特征空间上的间隔最大的线性分类器
## 线性可分支持向量机
给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离平面以及相应的分类决策函数称为线性可分支持向量机
## 函数间隔和几何间隔
函数间隔：对于给定的训练数据集T和超平面，关于T中所有样本点的函数间隔之最小值；</br>
几何间隔：对超平面的法向量加约束，使得间隔是确定的，这时的函数间隔就是几何间隔。
## 间隔最大化
# 非线性支持向量机和核函数
## 核技巧
通过一个非线性变换将输入空间对应一个特征空间，使得在输入空间中的超曲面模型对应与特征空间中的超平面模型。</br>
核函数：设x是输入空间，设h是特征空间，存在x到h的映射o，使得对所有xz，函数k满足
k=o(x)·o(z),则k为核函数，o（x）是映射函数</br>
## 正定核
通常说的核函数就是正定核函数。</br>
多项式核函数、高斯核函数、字符串核函数
# 提升方法
## AdaBoost算法
强可学习： 在概率近似正确（PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习他，并且正确率很高，该概念就是强可学习的；</br>
弱可学习：一个概念，存在一个多项式的学习算法能够学习他，学习的正确率仅仅比随机猜测略好，那么就称为弱可学习；</br>
给定一个二类分类的训练数据集T
adaboost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并且将这些分类器线性组合成一个强分类器。</br>
## 前向分步算法
## 提升树
# EM算法
迭代算法，每次迭代有两步，E步，求期望；M步，求极大；即期望极大算法（EM算法）。</br>
